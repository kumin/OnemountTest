{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### Create a program that produces a typed parquet file\n",
    "The parquet file result was saved in directory 'data/green_tripdata_2013-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('Onemount Test') \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.csv('data/green_tripdata_2013-09.csv', header = True)\n",
    "\n",
    "for column_name in df.columns:\n",
    "    df = df.withColumnRenamed(column_name, column_name.replace(\" \", \"\").lower())\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.repartition(1).write.mode('overwrite').parquet('data/green_tripdata_2013-09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Create a derived dataset, from the one created above\n",
    "The parquet file result was saved in directory 'data/nyc_taxi_analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encoding for each hour of the day\n",
    "Add 24 columns correspond to 24 hours of the day: **0_hour, 1_hour, 2_hour...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "df = spark.read.parquet('data/green_tripdata_2013-09')\n",
    "\n",
    "hour_of_day = F.udf(lambda pickup_hour, dropoff_hour, i: 1 if pickup_hour == i \n",
    "                    or dropoff_hour == i else 0, IntegerType())\n",
    "for i in range(24):\n",
    "    df = df.withColumn(str(i)+'_hour', hour_of_day(F.hour('lpep_pickup_datetime'), \n",
    "                                                   F.hour('lpep_dropoff_datetime'), \n",
    "                                                   F.lit(i)))\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT sum(3_hour) total_3_hour,sum(18_hour) total_18_hour,sum(15_hour) total_15_hour,sum(0_hour) total_0_hour FROM nyc_taxi\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encoding for each day\tof the week\n",
    "Add 7 columns correspond to 7 days of the week: **monday, tuesday, wednesday, thursday, friday, saturday, sunday**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "\n",
    "day_of_week = F.udf(lambda pickup_day, dropoff_day, d: 1 if pickup_day == d\n",
    "                    or dropoff_day == d else 0, IntegerType())\n",
    "for d in week:\n",
    "    df = df.withColumn(d, day_of_week(F.lower(F.date_format('lpep_pickup_datetime', 'EEEE')), \n",
    "                                                   F.lower(F.date_format('lpep_dropoff_datetime', 'EEEE')), \n",
    "                                                   F.lit(d)))\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT sum(monday) total_monday, sum(sunday) total_sunday FROM nyc_taxi\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration in seconds of the trip\n",
    "Add a column **trip_duration** type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timeDiff = F.unix_timestamp('lpep_dropoff_datetime')- F.unix_timestamp('lpep_pickup_datetime')\n",
    "df = df.withColumn('trip_duration', timeDiff)\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT lpep_pickup_datetime, lpep_dropoff_datetime, trip_duration FROM nyc_taxi\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An int encoding to indicate if the pickup or dropoff locations were at JFK airport\n",
    "Add a lolumn **relative_jfk_airport** type int: 1 is pick up or drop off at JFK airport else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no time for researching, so i copied it on internet\n",
    "import math\n",
    "def cal_bounding_box(longitude:float, latitude:float):\n",
    "    R = 6371  # earth radius in km\n",
    "    radius = 5 # km\n",
    "    min_lon = longitude - math.degrees(radius/R/math.cos(math.radians(latitude)))\n",
    "    max_lon = longitude + math.degrees(radius/R/math.cos(math.radians(latitude)))\n",
    "    min_lat = latitude - math.degrees(radius/R)\n",
    "    max_lat = latitude + math.degrees(radius/R)\n",
    "    \n",
    "    return min_lon, max_lon, min_lat, max_lat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude_jfk_airport = -73.7787443\n",
    "latitude_jfk_airport = 40.6398262\n",
    "\n",
    "min_lon, max_lon, min_lat, max_lat = cal_bounding_box(longitude_jfk_airport, latitude_jfk_airport)\n",
    "\n",
    "is_jfk_airport = F.udf(lambda p_lon, p_lat, d_lon, d_lat: \n",
    "                       1 if (min_lon<=p_lon and p_lon <= max_lon and min_lat <= p_lat and p_lat <= max_lat) \n",
    "                           or (min_lon<=d_lon and d_lon <= max_lon and min_lat <= d_lat and d_lat <= max_lat)\n",
    "                       else 0, IntegerType())\n",
    "\n",
    "df = df.withColumn('relative_jfk_airport', \n",
    "                   is_jfk_airport(df.pickup_longitude.cast(FloatType()), \n",
    "                                  df.pickup_latitude.cast(FloatType()), \n",
    "                                  df.dropoff_longitude.cast(FloatType()), \n",
    "                                  df.dropoff_latitude.cast(FloatType())))\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT count(*) no_jfk_airport FROM nyc_taxi WHERE relative_jfk_airport = 1\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the new parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').parquet('data/nyc_taxi_analysis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
