{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "### Create a program that produces a typed parquet file\n",
    "The parquet file result was saved in directory 'data/green_tripdata_2013-09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorid: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('Onemount Test') \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.csv('data/green_tripdata_2013-09.csv', header = True)\n",
    "\n",
    "for column_name in df.columns:\n",
    "    df = df.withColumnRenamed(column_name, column_name.replace(\" \", \"\").lower())\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.repartition(1).write.mode('overwrite').parquet('data/green_tripdata_2013-09')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Create a derived dataset, from the one created above\n",
    "The parquet file result was saved in directory 'data/nyc_taxi_analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encoding for each hour of the day\n",
    "Add 24 columns correspond to 24 hours of the day: **0_hour, 1_hour, 2_hour...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+------------+\n",
      "|total_3_hour|total_18_hour|total_15_hour|total_0_hour|\n",
      "+------------+-------------+-------------+------------+\n",
      "|        1116|         4381|         3216|        2463|\n",
      "+------------+-------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "df = spark.read.parquet('data/green_tripdata_2013-09')\n",
    "\n",
    "hour_of_day = F.udf(lambda pickup_hour, dropoff_hour, i: 1 if pickup_hour == i \n",
    "                    or dropoff_hour == i else 0, IntegerType())\n",
    "for i in range(24):\n",
    "    df = df.withColumn(str(i)+'_hour', hour_of_day(F.hour('lpep_pickup_datetime'), \n",
    "                                                   F.hour('lpep_dropoff_datetime'), \n",
    "                                                   F.lit(i)))\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT sum(3_hour) total_3_hour,sum(18_hour) total_18_hour,sum(15_hour) total_15_hour,sum(0_hour) total_0_hour FROM nyc_taxi\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot encoding for each day\tof the week\n",
    "Add 7 columns correspond to 7 days of the week: **monday, tuesday, wednesday, thursday, friday, saturday, sunday**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|total_monday|total_sunday|\n",
      "+------------+------------+\n",
      "|        7370|        7990|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "week = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "\n",
    "day_of_week = F.udf(lambda pickup_day, dropoff_day, d: 1 if pickup_day == d\n",
    "                    or dropoff_day == d else 0, IntegerType())\n",
    "for d in week:\n",
    "    df = df.withColumn(d, day_of_week(F.lower(F.date_format('lpep_pickup_datetime', 'EEEE')), \n",
    "                                                   F.lower(F.date_format('lpep_dropoff_datetime', 'EEEE')), \n",
    "                                                   F.lit(d)))\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT sum(monday) total_monday, sum(sunday) total_sunday FROM nyc_taxi\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration in seconds of the trip\n",
    "Add a column **trip_duration** type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+\n",
      "|lpep_pickup_datetime|lpep_dropoff_datetime|trip_duration|\n",
      "+--------------------+---------------------+-------------+\n",
      "| 2013-09-01 00:02:00|  2013-09-01 00:54:51|         3171|\n",
      "| 2013-09-01 00:02:34|  2013-09-01 00:20:59|         1105|\n",
      "| 2013-09-01 00:03:06|  2013-09-01 00:28:03|         1497|\n",
      "| 2013-09-01 00:03:30|  2013-09-01 00:23:02|         1172|\n",
      "| 2013-09-01 00:05:12|  2013-09-01 00:30:55|         1543|\n",
      "| 2013-09-01 00:05:18|  2013-09-01 00:23:31|         1093|\n",
      "| 2013-09-01 00:05:51|  2013-09-01 00:13:32|          461|\n",
      "| 2013-09-01 00:07:01|  2013-09-01 00:13:56|          415|\n",
      "| 2013-09-01 00:07:55|  2013-09-01 00:24:19|          984|\n",
      "| 2013-09-01 00:07:58|  2013-09-01 00:38:32|         1834|\n",
      "| 2013-09-01 00:08:30|  2013-09-01 00:13:58|          328|\n",
      "| 2013-09-01 00:09:25|  2013-09-01 00:27:14|         1069|\n",
      "| 2013-09-01 00:09:41|  2013-09-01 00:17:37|          476|\n",
      "| 2013-09-01 00:11:14|  2013-09-01 00:16:05|          291|\n",
      "| 2013-09-01 00:11:17|  2013-09-01 00:34:44|         1407|\n",
      "| 2013-09-01 00:11:26|  2013-09-01 00:21:45|          619|\n",
      "| 2013-09-01 00:11:46|  2013-09-01 00:27:51|          965|\n",
      "| 2013-09-01 00:15:01|  2013-09-01 00:26:34|          693|\n",
      "| 2013-09-01 00:15:27|  2013-09-01 00:24:30|          543|\n",
      "| 2013-09-01 00:15:40|  2013-09-01 00:23:37|          477|\n",
      "+--------------------+---------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeDiff = F.unix_timestamp('lpep_dropoff_datetime')- F.unix_timestamp('lpep_pickup_datetime')\n",
    "df = df.withColumn('trip_duration', timeDiff)\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT lpep_pickup_datetime, lpep_dropoff_datetime, trip_duration FROM nyc_taxi\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An int encoding to indicate if the pickup or dropoff locations were at JFK airport\n",
    "Add a lolumn **relative_jfk_airport** type int: 1 is pick up or drop off at JFK airport else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no time for researching, so i copied it on internet\n",
    "import math\n",
    "def cal_bounding_box(longitude:float, latitude:float):\n",
    "    R = 6371  # earth radius in km\n",
    "    radius = 5 # km\n",
    "    min_lon = longitude - math.degrees(radius/R/math.cos(math.radians(latitude)))\n",
    "    max_lon = longitude + math.degrees(radius/R/math.cos(math.radians(latitude)))\n",
    "    min_lat = latitude - math.degrees(radius/R)\n",
    "    max_lat = latitude + math.degrees(radius/R)\n",
    "    \n",
    "    return min_lon, max_lon, min_lat, max_lat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|no_jfk_airport|\n",
      "+--------------+\n",
      "|           893|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "longitude_jfk_airport = -73.7787443\n",
    "latitude_jfk_airport = 40.6398262\n",
    "\n",
    "min_lon, max_lon, min_lat, max_lat = cal_bounding_box(longitude_jfk_airport, latitude_jfk_airport)\n",
    "\n",
    "is_jfk_airport = F.udf(lambda p_lon, p_lat, d_lon, d_lat: \n",
    "                       1 if (min_lon<=p_lon and p_lon <= max_lon and min_lat <= p_lat and p_lat <= max_lat) \n",
    "                           or (min_lon<=d_lon and d_lon <= max_lon and min_lat <= d_lat and d_lat <= max_lat)\n",
    "                       else 0, IntegerType())\n",
    "\n",
    "df = df.withColumn('relative_jfk_airport', \n",
    "                   is_jfk_airport(df.pickup_longitude.cast(FloatType()), \n",
    "                                  df.pickup_latitude.cast(FloatType()), \n",
    "                                  df.dropoff_longitude.cast(FloatType()), \n",
    "                                  df.dropoff_latitude.cast(FloatType())))\n",
    "\n",
    "df.createOrReplaceTempView('nyc_taxi')\n",
    "df_sql = spark.sql('''\n",
    "    SELECT count(*) no_jfk_airport FROM nyc_taxi WHERE relative_jfk_airport = 1\n",
    "''')\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the new parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendorid: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- ratecodeid: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- 0_hour: integer (nullable = true)\n",
      " |-- 1_hour: integer (nullable = true)\n",
      " |-- 2_hour: integer (nullable = true)\n",
      " |-- 3_hour: integer (nullable = true)\n",
      " |-- 4_hour: integer (nullable = true)\n",
      " |-- 5_hour: integer (nullable = true)\n",
      " |-- 6_hour: integer (nullable = true)\n",
      " |-- 7_hour: integer (nullable = true)\n",
      " |-- 8_hour: integer (nullable = true)\n",
      " |-- 9_hour: integer (nullable = true)\n",
      " |-- 10_hour: integer (nullable = true)\n",
      " |-- 11_hour: integer (nullable = true)\n",
      " |-- 12_hour: integer (nullable = true)\n",
      " |-- 13_hour: integer (nullable = true)\n",
      " |-- 14_hour: integer (nullable = true)\n",
      " |-- 15_hour: integer (nullable = true)\n",
      " |-- 16_hour: integer (nullable = true)\n",
      " |-- 17_hour: integer (nullable = true)\n",
      " |-- 18_hour: integer (nullable = true)\n",
      " |-- 19_hour: integer (nullable = true)\n",
      " |-- 20_hour: integer (nullable = true)\n",
      " |-- 21_hour: integer (nullable = true)\n",
      " |-- 22_hour: integer (nullable = true)\n",
      " |-- 23_hour: integer (nullable = true)\n",
      " |-- monday: integer (nullable = true)\n",
      " |-- tuesday: integer (nullable = true)\n",
      " |-- wednesday: integer (nullable = true)\n",
      " |-- thursday: integer (nullable = true)\n",
      " |-- friday: integer (nullable = true)\n",
      " |-- saturday: integer (nullable = true)\n",
      " |-- sunday: integer (nullable = true)\n",
      " |-- trip_duration: long (nullable = true)\n",
      " |-- relative_jfk_airport: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.repartition(1).write.mode('overwrite').parquet('data/nyc_taxi_analysis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
